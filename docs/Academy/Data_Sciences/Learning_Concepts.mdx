---
sidebar_position: 3
---
# Notions d'apprentissage

Nous avons vu au chapitre pr√©c√©dent [Techniques Supervis√©es vs Non Supervis√©es](/Academy/Data_Sciences/Supervised_&_Unsupervised), que nous parlons d'apprentissage pour les techniques supervis√©es et non-supervis√©es.

Toutefois, la litt√©rature se r√©f√®re davantage aux techniques supervis√©es pour √©voquer les principes d'apprentissage. Cela a une certaine logique √©tant donn√© que dans le cadre de l'application d'une technique supervis√©e, nous, en tant que data scientist, jouons le r√¥le de professeur de la machine et dans cette d√©marche, nous l'entrainnons et nous v√©rifions si la machine a bien appris et si elle fait correctement son travail. A contrario, les techniques non-supervis√©es ne sont pas explicitement guid√©es dans l'apprentissage qui est enti√®rement autonome √† la machine et dont la finalit√© sera de mettre en avant des tendances ou des relations cach√©es que nous en tant que data scientist devront interpr√©ter comme √©tant utile ou non.

D'ailleurs un autre √©l√©ment fondamental qui distincte un apprentissage supervis√© d'un apprentissage non-supervis√© est le fait que la machine, dans un apprentissage supervis√©, assurera une continuit√© de cet apprentissage de mani√®re autonome par la suite. Nous avons vu comme exemple que si nous apprenons √† une machine que lorsqu'un email contient les mots "gratuit", "lotterie" et "argent", il y a une forte probabilit√© que ce soit un spam (courrier non d√©sirable), la machine pourrait d√©couvrir par la suite que le mot "bitcoin" est souvent reprit √©galement avec ces mots et donc sans que nous l'ayons explicitement programm√©e, elle int√©grera une nouvelle r√®gle : celle d'identifier le mot "bitcoin" comme ayant une certaine probabilit√© √† √™tre un spam. Dans le cadre d'un apprentissage non-supervis√©, √† chaque fois que l'on applique l'algorithme sur des nouvelles donn√©es, on obtiendra un nouveau mod√®le, c'est √† dire que la machine recommencera son apprentissage √† z√©ro.

## D√©roulement d'un apprentissage supervis√©

Pour r√©aliser un mod√®le de pr√©diction, nous devons disposer de donn√©es qui comprennent les valeurs que l‚Äôon souhaite pr√©dire. On parle de donn√©es ¬´¬†√©tiquet√©es¬†¬ª (labeled data). Nous devons donc disposer de colonnes consid√©r√©es comme √©tant des variables explicatives (des pr√©dicteurs, les X  : X<sub>1</sub>, X<sub>2</sub>,X<sub>3</sub>,...) ainsi qu'une variable √† expliquer (la variable pr√©dite Y).

### R√©partition d'un dataframe en variables explicatives et variable cible

La premi√®re √©tape d'un apprentissage supervis√©s et donc d'identifier et s√©lectionner les X et √©galement identifier la colonne Y. Bien entendu, nous partons de l'hypoth√®se que nous disposons de donn√©es "propres", c'est √† dire que toutes les √©tapes d'analyse et de pr√©paration (notamment des transformations) ont √©t√© r√©alis√©es et les donn√©es sont structur√©es et donc repr√©sent√©es sous forme tabulaire. Nous partons √©galement du postulat que la s√©lection des pr√©dicteurs - variables explicatives les plus utiles pour cr√©er notre mod√®le - a d√©j√† √©t√© r√©alis√© √©galement.

Nous disposons d'une table contenant des colonnes X et une colonne Y, comme suit :



    | X<sub>1</sub> | X<sub>2</sub> | X<sub>3</sub> | X<sub>...</sub> | X<sub>n</sub> |  Y |
    |:---------------------:|:----------:|:---------------------:|:---------------------:|:---------------------:|:------------------------:|
    | X<sub>1,1</sub> | X<sub>1,2</sub> | X<sub>1,3</sub>| ...| X<sub>1,n</sub> | Y<sub>1</sub>|
    | X<sub>2,1</sub> | X<sub>2,2</sub> | X<sub>2,3</sub>| ...| X<sub>n,n</sub> | Y<sub>2</sub>|
    | ... | ... | ...| ...| ... | ...|
    | X<sub>n,1</sub> | X<sub>n,2</sub> | X<sub>n,3</sub>| ...| X<sub>n,n</sub>| Y<sub>3</sub>|



import MyCustomTip from '@site/src/components/HomepageFeatures/MyCustomTip';

<MyCustomTip>
  X<sub>2,3</sub> correspond √† la valeur de la 2e ligne de la 3e colonne
</MyCustomTip>

Pour convertir nos donn√©es en X et Y, il suffit simplement de donner l'instruction des colonnes appartenant √† X et celle appartenant √† Y. L'√©criture [['En-t√™te Colonne 1','En-t√™te Colonne 2' ]] signifie qu'il y a plusieurs colonnes, alors que [ ] signifie qu'il n'y a qu'une seule colonne.

```python
import pandas as pd

df = pd.DataFrame(data)


X = df[['X1', 'X2', 'X3']]  
y = df['Y']
```

### Partitionnement des donn√©es

La seconde √©tape consiste √† subdiviser (partitionner) les enregistrements afin d'utiliser une partition pour cr√©er le mod√®le (apprendre √† la machine) et l'autre partition pour valider le mod√®le (v√©rifier que l'√©l√®ve, la machine, a bien appris).

On partitionne donc les enregistrements des colonnes X et Y en deux parties :

<ul>
  <li>Set d'entrainnement qui comprends les donn√©es d'apprentissage</li>
  <li>Set de validation qui comprends les donn√©es qui seront utilis√©es pour v√©rifier l'apprentissage</li>
</ul> 

Le partitionnement des donn√©es (al√©atoire) d√©pend fortement de la quantit√© de donn√©es √† dispositon et de la complexit√© de la technique qui sera utilis√©. Par exemple, si on dispose d'un nombre de donn√©es restreint, le partitionnement sera r√©alis√© comme suit : Set d'entrainnement 60 % ; Set de test 40 %.

Par contre, si on dispose de nombreuse donn√©es et que la technique requi√®re un nombre important d'enregistrements (par exemple un r√©seau de neurones), le partitionnement sera r√©alis√© comme suit : Set d'entrainnement 80 % ; Set de test 20 %.

> :bulb: **Tip:** La r√©partition des enregistrements entre la partition "entrainnement" et la partition "validation" doit √™tre r√©alis√©e de mani√®re al√©atoire 

Ce partitionnement peut √™tre r√©alis√© de mani√®re simple √† l'aide de la librairie sp√©cialis√©e en machine learning : sickit-learn :

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

X et y reprennent toutes nos donn√©es. X contient plusieurs colonnes et de nombreux enregistrements. Y ne contient qu'une colonne et le m√™me nombre d'enregistrements que X. La fonction train_test_split va nous permettre de r√©aliser ce partionnement et comprends ci-dessus 4 arguments. Le premier identifie les X, le second identifie y, le troisi√®me d√©finit la taille de la partition de test (dans ce cas-ci 20 %) et le quatri√®me, random_state permet de fixer le graine de g√©n√©rateur de nombre al√©atoire √† 42, de sorte qu'√† chaquefois que nous ex√©cutons la fonction train_test_split avec la m√™me valeur pour random_state, nous obtiendrons exactement la m√™me division de donn√©es. 

### √âtape d'apprentissage 

Dans la phase d'apprentissage, nous appliquons une technique statistique √† nos donn√©es d'entrainnement. Par exemple, pour la r√©gression lin√©aire, technique supervis√©e param√©trique, l'algorithme va r√©aliser un processus comprenant diff√©rentes √©tapes permettant de d√©finir les param√®tres, c'est √† dire les incidences des valeurs de X sur la valeur Y. Par exemple : l'incidence de la superficie en m¬≤ d'une maison sur sa valeur.

L'algorithme - qui consiste en une s√©rie d'√©tapes (y compris l'√©tape de partitionnement des donn√©es) - , va nous fournir un mod√®le que nous allons pouvoir appliquer √† de nouvelles donn√©es.

Voici un exemple d'apprentissage en appliquant un model de r√©gression lin√©aire pr√©d√©fini (Nous d√©composerons ce mod√®le dans le chapitre sur la r√©gression lin√©aire).

```python
model = LinearRegression()
model.fit(X_train, y_train)
```

### √âtape de validation

D√®s que nous obtenons un mod√®le, nous allons le v√©rifier, c'est √† dire le valider. La mani√®re la plus simple de r√©aliser une premi√®re validation est de v√©rifier si le mod√®le fonctionne correctement en lui appliquant les donn√©es sur base desquelles il a r√©alis√© son apprentissage, mais cette fois-ci, on va lui cacher les valeurs des y et lui demander de les pr√©dire. √©tant donn√© que nous utilisons les m√™mes donn√©es que celle utilis√©es lors de l'apprentissage, nous nous attendons √† obtenir √† cette √©tape-ci, un excellent score en v√©rifiant les valeurs pr√©dites par rapport aux valeurs r√©elles. L'id√©e est de s'assurer que ce mod√®le est robuste dans son apprentissage.

```python
y_pred = model.predict(X_train)
```

Nous utilisons la fonction de pr√©diction de notre mod√®le sur les m√™mes valeurs de X √† partir desquelles notre mod√®le a √©t√© cr√©√©.

Les erreurs de pr√©diction dans le set d‚Äôentrainement (validation) nous donnent une indication sur l‚Äôajustement du mod√®le (sous-entra√Æn√© ou surentrain√©). 


:::warning
L'erreur dans l'√©tape de validation, c'es √† dire la diff√©rence entre les valeurs pr√©dites et valeurs r√©elles doit √™tre minimale mais ne doit absolument pas correspondre √† 0. Si vous obtenez 0, c'est que votre mod√®le est en sur-apprentissage. Il connait trop bien ses donn√©es d'entrainnement et ne sera probablement pas g√©n√©ralisable et donc donnera un score m√©diocre sur de nouvelles donn√©es (par exemple lors de l'√©tape de test)
:::



### √âtape de test

Si l'√©tape de validation est concluante, c'est √† dire que nous obtenons une fiabilit√© √©lev√©e (faible diff√©rence entre les valeurs pr√©dites par le mod√®le et les valeurs r√©elles connues), nous passons √† la phase de test. Attention, si l'√©tape de validation m√®ne √† une fiabilit√© faible, cela signifie que le mod√®le est sous-entrainn√©. Il n'est pas capable de pr√©dire correctement des valeurs sur lesquelles il a √©t√© entrainn√© ! Il ne dispose pas de suffisamment d'enregistrements ou de colonnes et d'enregistrements. 

L'√©tape de test consiste √† appliquer notre mod√®le sur la seconde partition de nos donn√©es, celles √† partir desquelles notre mod√®le n'a pas √©t√© entrainn√© mais dont nous connaissons les valeurs des X et les valeurs des Y. Nous appliquons la fonction de pr√©diction sur les X et nous comparons les valeurs obtenues.



```python
y_pred = model.predict(X_test)
```
Nous v√©rifions ensuite la diff√©rence entre les valeurs pr√©dites et les valeurs r√©elles.

Le r√©sultat de l'√©tape de test devrait √™tre l√©g√®rement inf√©rieur √† l'√©tape de validation signifiant que notre mod√®le est capable de d√©finir une r√®gle g√©n√©ralisable. Sur base des donn√©es d'entrainnement il a √©t√© capable de g√©n√©raliser la r√®gle √† n'importe quelle donn√©es.


## √âvaluation d'un apprentissage supervis√©

Nous serons confront√© √† la n√©cessit√© d'utilis√© et √©valuer plusieurs techniques afin de pouvoir identifier celle qui nous permettra d'obtenir le mod√®le le plus efficient au regard de nos donn√©es. Selon la technique, nous seront √©galement amen√© √† √©valuer diff√©rentes configurations pouvant mener √† un meilleur ou moins bon apprentissage.

### R√®gle g√©n√©ralisable

 Les erreurs de pr√©diction dans le set d‚Äôentrainement (validation) nous donnent une indication sur l‚Äôajustement du mod√®le (sous-entra√Æn√© ou surentrain√©). Par exemple, si l‚Äôerreur d‚Äôentrainement est de 0, mon mod√®le est certainement en sur-apprentissage, il est en effet impossible d'obtenir un mod√®le parfait. Les erreurs de pr√©diction dans le set de validation devraient toutefois √™tre minimis√©es √©tant donn√© que notre mod√®le a √©t√© entrainn√© sur base des ces donn√©es. Si le set de validation pr√©sente un taux d'erreur plus important, il est fort probable que nous soyons en sous-apprentissage. Enfin, les erreurs de pr√©diction dans le set de test (erreurs pr√©dictives) mesurent la capacit√© de pr√©diction du mod√®le et devraient √™tre l√©g√®rement plus importante que dans le set de validation, sans pour autant s'en √©cart√© de mani√®re trop importante afin de d√©montrer la capacit√© du mod√®le √† avoir cr√©√© une r√®gle g√©n√©ralisable.


### Sous-apprentissage
<div class="todo">Ajouter un graphique ici
Graphique √† ajouter
Classement & estimation : https://images.app.goo.gl/JEnhJEv2YswHWfEL9
Le sous-apprentissage apparait lorsque Lorsque l‚Äô√©chantillon d‚Äôapprentissage ne couvre pas suffisamment de cas diff√©rents ou les variables sont insuffisantes en termes de quantit√©, le mod√®le peut avoir du mal √† g√©n√©raliser correctement.
En sous-apprentissage absolu, nous obtiendrons un mauvais r√©sultat lors de l'application de l'√©tape de validation. Toutefois nous pourrions √©galement rencontrer le cas o√π l'√©tape d'apprentissage obtiendrait un score qui parraitrait "bon" mais o√π l'application du mod√®le aux donn√©es de test renverrait un mauvais score.</div>



### Sur-apprentissage
<div class="todo">Ajouter un graphique ici
Graphique √† ajouter
Classement & estimation : https://images.app.goo.gl/JEnhJEv2YswHWfEL9
Le sur-apprentissage apparait surtout lorsque l‚Äô√©chantillon est trop important. Pour ajuster son mod√®le, il faut donc soit diminuer la taille de l‚Äô√©chantillon, soit le augmenter le nombre de variables explicatives. En sur-apprentissage absolu, nous obtenons un excellent r√©sultat lors de l'application de l'√©tape de  mais un mauvais score lors de l'application du mod√®le aux donn√©es de test renverrait un mauvais score.</div>


### √âvaluation d'un mod√®le d'estimation

Dans le cadre d'une estimation, les erreurs de pr√©diction dans le set d‚Äôentrainement (validation) nous donnent une indication sur l‚Äôajustement du mod√®le (sous-entra√Æn√© ou surentrain√©). Par exemple, si l‚Äôerreur d‚Äôentrainement est de 0, mon mod√®le est certainement en sur-apprentissage..Les erreurs dans le set validation doivent √™tre inf√©rieures aux erreurs dans le set de validation √©tant donn√© que le mod√®le a √©t√© entrain√© sur base du set d‚Äôentrainement (et donc du set de validation). 

**Une erreur $$(e)$$** est en r√©alit√© la diff√©rence entre la valeur r√©elle de $$y$$ et connue et la valeur pr√©dite par le mod√®le que l'on nomme $\hat{y}$

$$e = y-≈∑$$  On notera $$e_i = (ùë¶_i ‚àí ≈∑_i )$$ pour l'erreur d'un enregistrement sp√©cifique.

:::warning
Il est important de consid√©rer qu'il existe diff√©rents indicateurs d'√©valuation et sp√©cifiques selon la technique statistique utilis√©e. 
:::

L'√©valuation dans tous les cas, la litt√©rature privil√©gie l'utilisation de **l'erreur quadratique moyenne (root mean square error)**  :
<br /> 

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

<br /> 
Cette formule pourrait √©galement √™tre inscrite comme suit :
<br /> 

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (ei)^2
$$
<br /> 
o√π $e_i = y_i - \hat{y}_i$ (l'erreur de chaque enregistrement) et $n =$ le nombre total d'enregistrements
<br /> 
L'erreur quadratique moyenne (RMSE) est exprim√©e dans les m√™mes unit√©s que la variable cible. Si l'on essaie de pr√©dire le prix d'une maison et que l'on obtient un RMSE de 75.000 ‚Ç¨ et que l'√©chelle $min/max$ des prix des maisons de votre mod√®le est de $350.000 ‚Ç¨ - 500.000 ‚Ç¨$, il va de soi que le RMSE obtenu n'est pas optimal. Par contre si ces m√™mes valeurs des maisons de votre mod√®le sont de $1.950.000 ‚Ç¨ - 2.550.000 ‚Ç¨, un RMSE de 75.000 ‚Ç¨ est consid√©r√© comme faible et donc un bon mod√®le. L'√©valuation du RMSE doit tenir compte de l'√©chelle des donn√©es. 

Toutefois, il est important de souligner qu'un des principes de base d'un mod√®le d'apprentissage est de normaliser ou standardiser les donn√©es afin de mettre les donn√©es sur une √©chelle commune et rendre le mod√®le plus efficient. Par exemple, si nous souhaitons cr√©er un mod√®le sur l'ensemble de nos donn√©es comprenant tant des ventes d'appartements, de maisons que de villa haut de gamme, notre mod√®le contiendra de nombreux enregistrements am√©liorant la diversit√© et donc favorisant une r√®gle g√©n√©ralisable. Dans ce cas, nous souhaitons obtenur une valeur du RMSE proche de 0 indiquant un tr√®s bon ajustement. Attention toutefois √† consid√©rer qu'un RMSE correspondant √† 0 dans la phase de validation (application du mod√®le sur les m√™mes donn√©es que celles √† partir desquelles il a √©t√© entrainn√©), est signe d'un surapprentissage. Ce surapprentissage sera confirm√© si en phase de test, le RMSE obtenu est tr√®s √©lev√©. Nous souhaitons en effet obtenir un RMSE faible (tr√®s faible) en validation et en test un RMSE moins performant que la validation mais sans pour autant creuser un √©cart trop important.

### √âvaluation d'un mod√®le de classement

Dans le cadre d'un classement, notre mod√®le attribuera une classe √† la valeur pr√©dite. Prenons le cas d'un mod√®le de classement √† deux classes (exemple contagieux (1) / non contagieux (0)). $\hat{y}$ aura donc comme valeur finale possible 1 ou 0. Bien entendu, nous n'obtiendrons pas un mod√®le parfait. Par cons√©quent, pour certaines valeurs de $y = 1$, notre $\hat{y}$ correspondra parfois √† $1$ et parfois √† $0$.

Si nous avons plus de deux classes, par exemple Bon, moyen ou mauvais, $\hat{y}$ aura  comme valeurs finales possibles 1, 2 ou 3 et pour certaines valeurs de $y = 1$, notre $\hat{y}$ correspondra parfois √† $1$, parfois √† $2$ et parfois √† $3$.

Pour √©valuer un mod√®le de classement, nous cr√©ons une matrice - la matrice de confusion - dans laquelle nous r√©sumons les classements corrects et incorrects. Cette matrice nous permettra de calculer **le taux de pr√©cision** et/ou le **taux d'erreur global**.

Prenons l'exemple d'un mod√®le dont l'objectif est de pr√©dire pour $\hat{y}$ soit une valeur 1 soit une valeur 0. Dans ce cas, la matrice contiendra 4 cases comme suit :
<br /> 

|        | Y = 0     | Y = 1    |
|:-----:|:---------:|:--------:|
| ≈∑ = 0 |   <span class="text-green">55</span>  | <span class="text-red">3</span>     |
| ≈∑ = 1|    <span class="text-red">9</span>    | <span class="text-green">33</span>  |




Pour calculer le **taux de pr√©cision**, il suffit d'additionner les valeurs √©valu√©es correctement que nous divisons par le total de valeur.

$$
\text{Taux de pr√©cision} = \frac{\color{green}{55} + \color{green}{33}}{\color{green}{55} + \color{red}{3} + \color{red}{9} + \color{green}{33}} = \text{88 pourcent}
$$
<br /> 
Pour calculer l' **erreur globale**, il suffit de soustraire le taux de pr√©cision de 1 = 22 pourcent
<br /> 
$$
\text{Taux d'erreur globale} = 1 -\text{Taux de pr√©cision}
$$
<br /> 
Ce qui nous int√©resse, ce sont les pr√©dictions correctemment r√©alis√©es, √† savoir, les vrais n√©gatifs (les non-contagieux qui sont correctements pr√©dits comme non-contagieux) et les vrais positifs (les contagieux qui sont pr√©dits comme contagieux) contrairement aux faux positifs (les non-contagieux qui sont pr√©dits comme √©tant contagieux) et les faux n√©gatifs (les contagieux qui sont pr√©dits comme non-contagieux). Le matrice se pr√©sente de la mani√®re suivante :
<br /> 

|        | Y = 0 Non-Contagieux     | Y = 1  Contagieux  |
|:-----:|:---------:|:--------:|
| ≈∑ = 0 |   <span class="text-green">Vrais n√©gatifs</span>  | <span class="text-red">Faux n√©gatifs</span>     |
| ≈∑ = 1|    <span class="text-red">Faux positifs</span>    | <span class="text-green">Vrais n√©gatifs</span>  |


<br /> 
Dans notre cas, nous avant d√©finit un seuil √† $0.5$, √† savoir qu'un r√©sultat $>=0.5$ est consid√©r√© comme contagieux et un r√©sultat $<0.5$ est consid√©r√© comme non contagieux. Il est √©videmment plus prudent d'avoir un mod√®le qui pr√©dit davantage de personne comme √©tant contagieuses alors qu'elles ne le sont pas plut√¥t qu'un mod√®le qui pr√©dit davantage les personnes contagieuses comme √©tant non-contagieuses. Nous sommes libre √©videmment par prudence de r√©hausser le seuil par exemple √† $0.65$.

Si l'objectif de notre mod√®le est de pr√©dire plus de deux classes, par exemple : "Bon","Moyen","Mauvais'; dans ce cas, nous ajoutons √† notre matrice $n$ colonnes suppl√©mentaires, $n$ √©tant le nombre de classes √† pr√©dire.


<br /> 


|        | Y = 1 (Bon)  | Y = 2 (Moyen)    | Y = 3 (Mauvais)|
|:-----:|:---------:|:--------:|:--------:|
| ≈∑ = 1 (Bon) |   <span class="text-green">Vrais positifs "Bon"</span>  | <span class="text-red">Faux positifs "Bon" <br /> & Faux n√©gatifs "Moyen"</span>     | <span class="text-red">Faux positifs "Bon" <br /> & Faux n√©gatifs "Mauvais"</span>  |
| ≈∑ = 2 (Moyen)|    <span class="text-red">Faux positifs "Moyen" <br /> & Faux n√©gatifs "Bon"</span>    | <span class="text-green">Vrais positifs "Moyen" </span>  |<span class="text-red">Faux positifs "Moyen" <br /> & Faux n√©gatifs "Mauvais"</span>  |
| ≈∑ = 3 (Mauvais)|   <span class="text-red">Faux positifs "Mauvais" <br /> & Faux n√©gatifs "Bon"</span>    | <span class="text-red">Faux positifs "Mauvais" <br /> & Faux n√©gatifs "Moyen"</span>   |<span class="text-green">Vrais positifs "Mauvais" </span> |

<br /> 

De nouveau, pour calculer le taux de pr√©cision, il suffit d'additionner toutes les valeurs correctes et de les diviser par le total d'enregistrements.